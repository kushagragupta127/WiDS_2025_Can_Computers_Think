{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt",
    "N_BANDITS = len(bandits)\n",
    "N_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of ten bandit objects initialized in the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [Bandit(random.random()*4 - 2) for _ in range(10)]\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy():\n",
    "    Q = np.zeros(N_BANDITS)\n",
    "    N = np.zeros(N_BANDITS)\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(N_ITER):\n",
    "        if t < N_BANDITS:\n",
    "            action = t\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        reward = bandits[action].pullLever()\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = run_greedy()\n",
    "cum_avg = np.cumsum(rewards) / np.arange(1, N_ITER + 1)\n",
    "\n",
    "plt.plot(cum_avg)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Average Reward\")\n",
    "plt.title(\"Greedy Algorithm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epsilon_greedy(epsilon):\n",
    "    Q = np.zeros(N_BANDITS)\n",
    "    N = np.zeros(N_BANDITS)\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(N_ITER):\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, N_BANDITS - 1)\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        reward = bandits[action].pullLever()\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases but for various values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.01, 0.1, 0.3]\n",
    "\n",
    "for eps in epsilons:\n",
    "    rewards = run_epsilon_greedy(eps)\n",
    "    cum_avg = np.cumsum(rewards) / np.arange(1, N_ITER + 1)\n",
    "    plt.plot(cum_avg, label=f\"ε = {eps}\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"ε-Greedy Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "for eps in epsilons:\n",
    "    rewards = run_epsilon_greedy(eps)\n",
    "    cum_avg = np.cumsum(rewards) / np.arange(1, N_ITER + 1)\n",
    "    plt.plot(cum_avg, label=f\"ε = {eps}\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Finding Optimal ε\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimistic_greedy(Q_init=10):\n",
    "    Q = np.ones(N_BANDITS) * Q_init\n",
    "    N = np.zeros(N_BANDITS)\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(N_ITER):\n",
    "        action = np.argmax(Q)\n",
    "        reward = bandits[action].pullLever()\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases for an optimistic greedy of $Q_1 = 10$ and a non-optimistic $\\epsilon = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rewards = run_optimistic_greedy()\n",
    "eps_rewards = run_epsilon_greedy(0.1)\n",
    "\n",
    "plt.plot(np.cumsum(opt_rewards) / np.arange(1, N_ITER + 1), label=\"Optimistic Q₁ = 10\")\n",
    "plt.plot(np.cumsum(eps_rewards) / np.arange(1, N_ITER + 1), label=\"ε = 0.1\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Optimistic Initialization vs ε-Greedy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucb(c):\n",
    "    Q = np.zeros(N_BANDITS)\n",
    "    N = np.zeros(N_BANDITS)\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(N_ITER):\n",
    "        if t < N_BANDITS:\n",
    "            action = t\n",
    "        else:\n",
    "            ucb_values = Q + c * np.sqrt(np.log(t + 1) / (N + 1e-5))\n",
    "            action = np.argmax(ucb_values)\n",
    "\n",
    "        reward = bandits[action].pullLever()\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = run_ucb(c=2)\n",
    "cum_avg = np.cumsum(rewards) / np.arange(1, N_ITER + 1)\n",
    "\n",
    "plt.plot(cum_avg)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Average Reward\")\n",
    "plt.title(\"UCB Algorithm (c = 2)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
